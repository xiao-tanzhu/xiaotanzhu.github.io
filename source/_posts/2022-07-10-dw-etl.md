---
layout: post
title: 数据仓库【十】：ETL
date: 2022-07-10
tags:
- Data Warehouse
- 数据仓库
categories: 数据仓库
description: ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程，是数据仓库的生命线。
---

## 什么是ETL

ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程，是数据仓库的生命线。

### 抽取（Extract）

主要是针对各个业务系统及不同服务器的分散数据，充分理解数据定义后，规划需要的数据源及数据定义，制定可操作的数据源，制定增量抽取和缓慢渐变的规则。

### 转换（transform）

主要是针对数据仓库建立的模型，通过一系列的转换来实现将数据从业务模型到分析模型，通过ETL工具可视化拖拽操作可以直接使用标准的内置代码片段功能、自定义脚本、函数、存储过程以及其他的扩展方式，实现了各种复杂的转换，并且支持自动分析日志，清楚的监控数据转换的状态并优化分析模型。

### 装载（Load）

主要是将经过转换的数据装载到数据仓库里面，可以通过直连数据库的方式来进行数据装载，可以充分体现高效性。在应用的时候可以随时调整数据抽取工作的运行方式，可以灵活的集成到其他管理系统中。

## ETL & ELT

伴随着数据仓库的发展（传送门：[数据仓库的八个发展阶段](/数据仓库/2022-07-02-dw-history.html)），数据量从小到大，数据实时性从T+1到准实时、实时，ETL也在不断演进。

**在传统数仓中**，数据量小，计算逻辑相对简单，我们可以直接用ETL工具实现数据转换（T），转换之后再加载到目标库，即（Extract-Transform-Load）。但**在大数据场景下**，数据量越大越大，计算逻辑愈发复杂，数据清洗需放在运算能力更强的分布式计算引擎中完成，ETL也就变成了ELT（Extract-Load-Transform）。

即：
```
Extract-Transform-Load  >>  Extract-Load-Transform
```

通常我们所说的ETL，已经泛指数据同步、数据清洗全过程，而不仅限于数据的抽取-转换-加载。

## 常用的ETL工具

下面将介绍几类ETL工具（sqoop，DataX，Kettle，canal，StreamSets）。

### sqoop

- 是Apache开源的一款在Hadoop和关系数据库服务器之间传输数据的工具。
- 可以将一个关系型数据库（MySQL ,Oracle等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导出到关系型数据库中。
- sqoop命令的本质是转化为MapReduce程序。
- sqoop分为导入（import）和导出（export），
- 策略分为table和query
- 模式分为增量和全量。

![](/images/0071.png)

![](/images/0001.jfif)

### DataX

DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。

![](/images/0072.png)

### Kettle

一款国外免费开源的、可视化的、功能强大的ETL工具，纯java编写，可以在Windows、Linux、Unix上运行，数据抽取高效稳定。

### canal

canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据实时订阅和消费，目前主要支持了MySQL，也支持mariaDB。

![](/images/0073.png)

### StreamSets

是大数据实时采集ETL工具，可以实现不写一行代码完成数据的采集和流转。通过拖拽式的可视化界面，实现数据管道(Pipelines)的设计和定时任务调度。

创建一个Pipelines管道需要配置数据源(Origins)、操作(Processors)、目的地(Destinations)三部分。

## ETL加载策略

### 增量

有些表巨大，我们需要选择增量策略，新增delta数据需要和存量数据merge合并。

只有新增（full join。能拿更新表就拿更新表）

![](/images/0074.png)

### 新增+删除

```
history-table Left join delet-table where delta-table.value is null == 表a
```
表a full join update-table (能拿update就拿update)

### 全量

每天一个全量表，也可一个hive天分区一个全量。

### 流式

使用kafka，消费mysql binlog日志到目标库，源表和目标库是1：1的镜像。

无论是全量还是增量的方式，都会浪费多余的存储或通过计算去重，得到最新的全量数据。为解决这一问题，西红柿墙裂建议kafka的数据同步方案，源表变化一条，目标表消费一条，目标表数据始终是一份最新全量数据，且为实时同步的。

ps.极端情况下可能会丢数，需要写几个监控脚本（详见上文数据质量部分）和补数脚本即可~
